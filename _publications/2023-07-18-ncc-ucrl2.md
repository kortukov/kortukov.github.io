---
title: "Contextual Multi-Armed Bandit with Costly Feature Observation in Non-stationary Environments"
type: Journal
collection: publications
permalink: /publication/2023-07-18-ncc-ucrl2
excerpt: 'We study contextual multi-armed bandit problem where the features are costly and the agent has to
    simultaneously learn the reward distributions and the feature importances. The environment undergoes distribution
    shifts making the problem more challenging.'
date: 2023-07-18
paperurl: 'https://ieeexplore.ieee.org/document/10502231'
codeurl: 'https://github.com/saeedghoorchian/NCC-Bandits'
authors: 'Saeed Ghoorchian, **Evgenii Kortukov**, Setareh Maghsudi'
venue: 'IEEE Open Journal of Signal Processing'
bibtex: "@ARTICLE{10502231, <br>
  author={Ghoorchian, Saeed and Kortukov, Evgenii and Maghsudi, Setareh}, <br>
  journal={IEEE Open Journal of Signal Processing},  <br>
  title={Contextual Multi-Armed Bandit with Costly Feature Observation in Non-stationary Environments},  <br>
  year={2024}, <br>
  volume={}, <br>
  number={}, <br>
  pages={1-12}, <br>
  keywords={Costs;Signal processing algorithms;Vectors;Decision making;Solid modeling;Signal processing;COVID-19;Contextual multi-armed bandit;non-stationary process;online learning;costly information acquisition}, <br>
  doi={10.1109/OJSP.2024.3389809} <br>
  }"
---
Maximizing long-term rewards is the primary goal in sequential decision-making problems. The majority of existing methods assume that side information is freely available, enabling the learning agent to observe all features' states before making a decision. In real-world problems, however, collecting beneficial information is often costly. That implies that, besides individual arms' reward, learning the observations of the features' states is essential to improve the decision-making strategy. The problem is aggravated in a non-stationary environment where reward and cost distributions undergo abrupt changes over time. To address the aforementioned dual learning problem, we extend the contextual bandit setting and allow the agent to observe subsets of features' states. The objective is to maximize the long-term average gain, which is the difference between the accumulated rewards and the paid costs on average. Therefore, the agent faces a trade-off between minimizing the cost of information acquisition and possibly improving the decision-making process using the obtained information. To this end, we develop an algorithm that guarantees a sublinear regret in time. Numerical results demonstrate the superiority of our proposed policy in a real-world scenario.

[<i class="fa fa-fw fa-book" aria-hidden="true"></i>Full paper](https://arxiv.org/abs/2307.09388) &nbsp;&nbsp;
[<i class="fa fa-fw fa-globe" aria-hidden="true"></i>Code](https://github.com/saeedghoorchian/NCC-Bandits)