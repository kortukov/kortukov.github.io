---
title: "ASIDE: Architectural Separation of Instructions and Data in Language Models"
type: Workshop
collection: publications
permalink: /publication/2025-03-05-aside
excerpt: 'We introduce an architectural change to LLMs that separates instructions from data, to improve their security.'
date: 2025-03-05
paperurl: 'https://openreview.net/forum?id=GlmqRQsCaI'
authors: 'Egor Zverev, **Evgenii Kortukov**, Alexander Panfilov, Soroush Tabesh, Sebastian Lapuschkin, Wojciech Samek, Christoph H. Lampert'
venue: 'ICLR 2025 Workshop Building Trust in LLMs'
bibtext: "@inproceedings{ <br>
    zverev2025aside,
    title={{ASIDE}: Architectural Separation of Instructions and Data in Language Models}, <br>
    author={Egor Zverev and Evgenii Kortukov and Alexander Panfilov and Soroush Tabesh and Sebastian Lapuschkin and Wojciech Samek and Christoph H. Lampert}, <br>
    booktitle={ICLR 2025 Workshop on Building Trust in Language Models and Applications}, <br>
    year={2025}, <br>
    url={https://openreview.net/forum?id=GlmqRQsCaI} <br>
}"
---
Despite their remarkable performance, large language models lack elementary safety features, and this makes them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause for the success of prompt injection attacks. In this work, we propose an architectural change, ASIDE, that allows the model to clearly separate between instructions and data by using separate embeddings for them. Specifically, the data embedding is initialized with a rotation of the pretrained modelâ€™s embedding, prompting the model to learn to treat instructions and data differently. We demonstrate the effectiveness of our method by showing (1) greatly increased instruction-data separation scores without a loss in model capabilities and (2) competitive results on prompt injection benchmarks, even without dedicated safety training. Additionally, we study the working mechanism behind our method through an analysis of model representations.
[<i class="fa fa-fw fa-book" aria-hidden="true"></i>Full paper](https://openreview.net/forum?id=GlmqRQsCaI) &nbsp;&nbsp;